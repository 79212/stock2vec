{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stock2vec\n",
    "\n",
    "Create vectors for stocks based on their relative volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import math\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from functools import partial\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the diluted earnings per share by ticker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "\n",
    "dataset_folder_path = 'input'\n",
    "dataset_filename = 'input/prices.csv'\n",
    "dataset_name = 'Prices'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(dataset_filename):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc=dataset_name) as pbar:\n",
    "        urlretrieve(\n",
    "            'https://s3.amazonaws.com/perl-ml/prices.csv?response-content-disposition=attachment&X-Amz-Security-Token=FQoDYXdzEN3%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaDDGDXIQxfhjhlalnoyKqAiqteedReEObibGFinGZUTbCNLqOsBrBfhb3m%2B9WSc202KdlXdoi8bxYATvctErFAeNF%2FlVgdPlu%2BRy8dLOHw5a%2BvTNM92V8V1XiJnuYgpe69GI914L1xceQGmcJ9qQ1Fg2iSi5cGj2%2FNL26CHIOmdblBGp6VUFUqtu0ZoRb18XXYBlSGQIGk4kxGfwiN5%2BbnQNB%2FInBx0YkDI5XFOIOXa1HzF4anoHgoSSjwdq8FXLQh8LXD5mYvqkTLokIssfZeJrc4TyPy9gZW4hewwbI4NAauQvJfde2Z%2BA%2B5iV4%2B%2B8wFFcDMeM%2Fg%2BYyrTVhaRVZ%2FIU033J6CXshjaL0uHwFleXw%2FHlzMjQst2YZmQu0EqxNCowwwxcugVsKcMaPdMq%2BWJ66qWxN5DZcC3oo%2FJvbxgU%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20170325T201144Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAIZGCD6XQ355X2AMA%2F20170325%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=0c2703d3dbef5f58006a3b7e89ff85b2b86e67542b861784fbff2da48434e0df',\n",
    "            dataset_filename,\n",
    "            pbar.hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "rows: 10000000it [00:14, 668326.67it/s]                            \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         adj_close       date ticker  epsdil         pe\n",
      "45526    13.745073 2001-06-18   OLED   -0.87 -15.798935\n",
      "3046709   6.147269 2001-06-18    YUM    0.69   8.909085\n",
      "1778402  43.419875 2001-06-18    SWY    2.31  18.796483\n",
      "315864   29.645033 2001-06-18    PEP    1.50  19.763356\n",
      "1743245  11.213455 2001-06-18    SVU    0.47  23.858415\n"
     ]
    }
   ],
   "source": [
    "chunksize = 1000000\n",
    "price_rows = 9191528\n",
    "\n",
    "price_reader = pd.read_csv('input/prices.csv', \n",
    "                      header=None,\n",
    "                      parse_dates=[1],\n",
    "                      chunksize=chunksize, \n",
    "                      iterator=True)\n",
    "\n",
    "df_prices = pd.DataFrame()\n",
    "\n",
    "with tqdm(total=price_rows, desc='rows') as pbar:\n",
    "    for chunk in price_reader:\n",
    "        df_prices = df_prices.append(chunk)\n",
    "        pbar.update(chunksize);\n",
    "\n",
    "df_prices.columns = ['adj_close', 'date', 'ticker', 'epsdil', 'pe']\n",
    "\n",
    "# Sort by date, then ticker\n",
    "df_prices.sort_values(['date', 'pe'], inplace=True)\n",
    "\n",
    "print(df_prices.head())\n",
    "\n",
    "prices = df_prices['adj_close'].values.tolist()\n",
    "dates = df_prices['date'].values.tolist()\n",
    "tickers = df_prices['ticker'].values.tolist()\n",
    "pes = df_prices['pe'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build context\n",
    "\n",
    "For each stock, find C stocks that have the closest volatility to that ticker for that day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window for 0 OLED 0\n",
      "1 YUM\n",
      "window for 9 SVU 2\n",
      "3 SWY\n",
      "4 PEP\n",
      "window for 18 PEP 4\n",
      "3 SWY\n"
     ]
    }
   ],
   "source": [
    "ticker_to_int = {}\n",
    "int_to_ticker = {}\n",
    "\n",
    "def get_ticker_int(idx):\n",
    "    ticker = tickers[idx]\n",
    "    key = ticker_to_int.get(ticker, None)\n",
    "    if key is None:\n",
    "        key = ticker_to_int[ticker] = len(ticker_to_int)\n",
    "        int_to_ticker[key] = ticker\n",
    "    return key\n",
    "\n",
    "def get_window(idx, total, window_size=5):\n",
    "    R = np.random.randint(1, window_size+1)\n",
    "    start = idx - R if (idx - R) > 0 else 0\n",
    "    stop = idx + R if (idx + R) < total else total\n",
    "\n",
    "    stock_int = get_ticker_int(idx)\n",
    "    stock_date = dates[idx]\n",
    "    \n",
    "    window = []\n",
    "    \n",
    "    for i in range(start, stop):\n",
    "        nearby_stock_int = get_ticker_int(i)\n",
    "        nearby_stock_date = dates[i]\n",
    "        if nearby_stock_int != stock_int and nearby_stock_date == stock_date:\n",
    "            window.append(nearby_stock_int)\n",
    "    \n",
    "    return window\n",
    "\n",
    "for idx in range(0, 20, 9):\n",
    "    print('window for', idx, tickers[idx], get_ticker_int(idx))\n",
    "    for nearby_int in get_window(idx, len(tickers), 5):\n",
    "        print(nearby_int, int_to_ticker[nearby_int])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9191/9191 [01:23<00:00, 106.73it/s]"
     ]
    }
   ],
   "source": [
    "batch_size = 1000\n",
    "window_size = 10\n",
    "\n",
    "total_prices = len(prices)\n",
    "\n",
    "pbar = tqdm(total=int(total_prices / batch_size))\n",
    "\n",
    "def get_batch(start):\n",
    "    x, y = [], []\n",
    "\n",
    "    stop = start + batch_size if (start + batch_size) < total_prices else total_prices\n",
    "\n",
    "    for i in range(start, stop):\n",
    "        batch_x = get_ticker_int(i)\n",
    "        batch_y = get_window(i, total_prices, window_size)\n",
    "        y.extend(batch_y)\n",
    "        x.extend([batch_x]*len(batch_y))\n",
    "\n",
    "    pbar.update();\n",
    "\n",
    "    return [x, y]\n",
    "\n",
    "def get_batches():\n",
    "    batches = []\n",
    "    \n",
    "    for start in range(0, total_prices, batch_size):\n",
    "        batches.append(get_batch(start))\n",
    "   \n",
    "    return batches\n",
    "\n",
    "batches = get_batches()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_stocks = len(df_prices['ticker'].unique())\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    inputs = tf.placeholder(tf.int32, [None], name='inputs')\n",
    "    labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "\n",
    "n_embedding = 400 # Number of embedding features \n",
    "\n",
    "with train_graph.as_default():\n",
    "    embedding = tf.Variable(tf.random_uniform((n_stocks, n_embedding), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "          \r",
      "9192it [01:40, 106.73it/s]"
     ]
    }
   ],
   "source": [
    "# Number of negative labels to sample\n",
    "n_sampled = 100\n",
    "\n",
    "with train_graph.as_default():\n",
    "    softmax_w = tf.Variable(tf.truncated_normal((n_stocks, n_embedding), stddev=0.1))\n",
    "    softmax_b = tf.Variable(tf.zeros(n_stocks))\n",
    "    \n",
    "    # Calculate the loss using negative sampling\n",
    "    loss = tf.nn.sampled_softmax_loss(softmax_w, softmax_b, \n",
    "                                      labels, embed,\n",
    "                                      n_sampled, n_stocks)\n",
    "    \n",
    "    cost = tf.reduce_mean(loss)\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with train_graph.as_default():\n",
    "    ## From Thushan Ganegedara's implementation\n",
    "    valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "    valid_window = 100\n",
    "    # pick 8 samples from (0,100) and (1000,1100) each ranges. lower id implies more frequent \n",
    "    valid_examples = np.array(random.sample(range(valid_window), valid_size//2))\n",
    "    valid_examples = np.append(valid_examples, \n",
    "                               random.sample(range(1000,1000+valid_window), valid_size//2))\n",
    "\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    \n",
    "    # We use the cosine distance:\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keep_dims=True))\n",
    "    normalized_embedding = embedding / norm\n",
    "    valid_embedding = tf.nn.embedding_lookup(normalized_embedding, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embedding, tf.transpose(normalized_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Iteration: 100 Avg. Training loss: 4.7353 0.1324 sec/batch\n",
      "Epoch 1/10 Iteration: 200 Avg. Training loss: 4.3297 0.1308 sec/batch\n",
      "Epoch 1/10 Iteration: 300 Avg. Training loss: 3.9655 0.1325 sec/batch\n",
      "Epoch 1/10 Iteration: 400 Avg. Training loss: 3.5779 0.1372 sec/batch\n",
      "Epoch 1/10 Iteration: 500 Avg. Training loss: 3.4165 0.1408 sec/batch\n",
      "Epoch 1/10 Iteration: 600 Avg. Training loss: 3.3865 0.1415 sec/batch\n",
      "Epoch 1/10 Iteration: 700 Avg. Training loss: 3.4901 0.1380 sec/batch\n",
      "Epoch 1/10 Iteration: 800 Avg. Training loss: 3.3706 0.1374 sec/batch\n",
      "Epoch 1/10 Iteration: 900 Avg. Training loss: 3.1838 0.1338 sec/batch\n",
      "Epoch 1/10 Iteration: 1000 Avg. Training loss: 3.2497 0.1291 sec/batch\n",
      "Nearest to ACAS: LCI, MCP, AI, AIT, FLR, CTWS, BWLD, GNE,\n",
      "Nearest to CLFD: A, GRT, USM, ITMN, NMRX, ESIO, CAS, LMIA,\n",
      "Nearest to CSC: FITB, RFP, JAH, EQT, AME, ACAD, AAMC, CBT,\n",
      "Nearest to RVLT: AMAG, PES, CPWR, HP, PLUG, DTLK, RNWK, DXLG,\n",
      "Nearest to FLS: SYK, QGEN, DIS, CUR, INTC, TIF, NEM, ALEX,\n",
      "Nearest to GGG: TGT, STZ, UHS, FBNK, ACHC, FINL, AFAM, MAIN,\n",
      "Nearest to PRXL: PNRA, GTS, EA, SYX, BBY, BSET, HTA, JBLU,\n",
      "Nearest to SPTN: FC, EGHT, TWI, SUP, CLH, CMS, GLW, CUBI,\n",
      "Nearest to OFG: NL, AFOP, AXS, WGO, EPR, GLAD, SCG, CEVA,\n",
      "Nearest to ASTE: CRK, SEAC, BDE, STAA, NPK, PPO, HXL, DE,\n",
      "Nearest to BGC: EDIG, TIVO, PRKR, NYLD, SWC, OSUR, PLPC, KTOS,\n",
      "Nearest to NANO: SGEN, RELL, EDIG, VGR, HLIT, ENR, CAMP, NDAQ,\n",
      "Nearest to MTD: XLS, FLTX, MLI, DIS, MGNX, SCCO, BGFV, POWL,\n",
      "Nearest to LLTC: HDNG, HLX, VAR, WOOF, LNN, CLVS, CAKE, EFII,\n",
      "Nearest to TPC: CFNB, CLI, PEG, NWSA, FCX, HOT, FBC, SANM,\n",
      "Nearest to ORLY: ACC, RSTI, DHR, FLR, WY, ORIT, MTD, CWST,\n",
      "Epoch 1/10 Iteration: 1100 Avg. Training loss: 3.4303 0.1285 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "\n",
    "with train_graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    iteration = 1\n",
    "    loss = 0\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for e in range(1, epochs+1):\n",
    "        start = time.time()\n",
    "        for batch in batches:\n",
    "            x = batch[0]\n",
    "            y = batch[1]\n",
    "            \n",
    "            feed = {inputs: x,\n",
    "                    labels: np.array(y)[:, None]}\n",
    "            train_loss, _ = sess.run([cost, optimizer], feed_dict=feed)\n",
    "            \n",
    "            loss += train_loss\n",
    "            \n",
    "            if iteration % 100 == 0: \n",
    "                end = time.time()\n",
    "                print(\"Epoch {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Avg. Training loss: {:.4f}\".format(loss/100),\n",
    "                      \"{:.4f} sec/batch\".format((end-start)/100))\n",
    "                loss = 0\n",
    "                start = time.time()\n",
    "            \n",
    "            if iteration % 1000 == 0:\n",
    "                # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "                sim = similarity.eval()\n",
    "                for i in range(valid_size):\n",
    "                    valid_stock = int_to_ticker[valid_examples[i]]\n",
    "                    top_k = 8 # number of nearest neighbors\n",
    "                    nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                    log = 'Nearest to %s:' % valid_stock\n",
    "                    for k in range(top_k):\n",
    "                        try:\n",
    "                            close_stock = int_to_ticker[nearest[k]]\n",
    "                            log = '%s %s,' % (log, close_stock)\n",
    "                        except Exception:\n",
    "                            print('nearest[k]', nearest[k])\n",
    "                    print(log)\n",
    "            \n",
    "            iteration += 1\n",
    "    save_path = saver.save(sess, \"checkpoints/text8.ckpt\")\n",
    "    embed_mat = sess.run(normalized_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
