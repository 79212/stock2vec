{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stock2vec\n",
    "\n",
    "Create vectors for stocks based on their relative volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import datetime\n",
    "import math\n",
    "import multiprocessing as mp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "import time\n",
    "from functools import partial\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the diluted earnings per share by ticker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "CSV_URL = 'https://s3.amazonaws.com/perl-ml/prices.csv?response-content-disposition=attachment&X-Amz-Security-Token=FQoDYXdzECIaDLG1ZU6Yzztd7CsNGCKsAgNa3zgOVIw%2BQB8y%2FcRAMdAYK0ZPWW59OqVSuRuFGv3NEX3LapeZnns4VZleRraw1352r%2BP1CJm2hqgg2OlGcjf8pa414x90CDCdyIemO8HJwoIr4nKi18945ZmxthTL04BJsHD1MN0Tp%2F30A3kUMqscJP68vuQ75w098gKBJFxlnKztFUnP91Myn3%2FrrNUKQ%2F%2BODJx%2Bmpu7CMOGZlDLlSHtpTKbo8pULbHFGZAe%2BAvPqq0KU71nJ%2FWjUPcbLaEjSxOZl3%2BP98cePjijlMC8O6r9JzjTqGKUUUiqOWA92QZ6UtZfUlkyO%2BcNdLGltRJrCkGEctmyhJ6Qnim0eIfSBlzhDVPAtuAdTDrXzi2d3SGOJNm8P56ak71Vnk7P%2FSyGZsdQ9G0nMXBH1GeG5yjr7ebGBQ%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20170328T010700Z&X-Amz-SignedHeaders=host&X-Amz-Expires=300&X-Amz-Credential=ASIAJBTQPDQAOL557TLA%2F20170328%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Signature=16a879624653ee25590a42768d975982001f3451249973af25e9d93942fec054'\n",
    "FILE_NAME = 'input/prices.csv'\n",
    "LOG_DIR = 'output'\n",
    "MODEL_PATH = os.path.join(LOG_DIR, \"model.ckpt\")\n",
    "STOCK_PATH = os.path.join(LOG_DIR,'stock.tsv')\n",
    "\n",
    "if not os.path.exists(LOG_DIR):\n",
    "    os.makedirs(LOG_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "\n",
    "if not os.path.exists('input'):\n",
    "    os.makedirs('input')\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile(FILE_NAME):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='Prices') as pbar:\n",
    "        urlretrieve(CSV_URL, FILE_NAME, pbar.hook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "chunksize = 1000000\n",
    "price_rows = 9191528\n",
    "\n",
    "price_reader = pd.read_csv('input/prices.csv', \n",
    "                      header=None,\n",
    "                      parse_dates=[1],\n",
    "                      chunksize=chunksize, \n",
    "                      iterator=True)\n",
    "\n",
    "df_prices = pd.DataFrame()\n",
    "\n",
    "with tqdm(total=price_rows, desc='rows') as pbar:\n",
    "    for chunk in price_reader:\n",
    "        df_prices = df_prices.append(chunk)\n",
    "        pbar.update(chunksize);\n",
    "\n",
    "df_prices.columns = ['adj_close', 'date', 'ticker', 'epsdil', 'pe']\n",
    "\n",
    "# Sort by date, then ticker\n",
    "df_prices.sort_values(['date', 'pe'], inplace=True)\n",
    "\n",
    "print(df_prices.head())\n",
    "\n",
    "prices = df_prices['adj_close'].values.tolist()\n",
    "dates = df_prices['date'].values.tolist()\n",
    "tickers = df_prices['ticker'].values.tolist()\n",
    "pes = df_prices['pe'].values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build context\n",
    "\n",
    "For each stock, find C stocks that have the closest volatility to that ticker for that day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "ticker_to_int = {}\n",
    "int_to_ticker = {}\n",
    "\n",
    "def get_ticker_int(idx):\n",
    "    ticker = tickers[idx]\n",
    "    key = ticker_to_int.get(ticker, None)\n",
    "    if key is None:\n",
    "        key = ticker_to_int[ticker] = len(ticker_to_int)\n",
    "        int_to_ticker[key] = ticker\n",
    "    return key\n",
    "\n",
    "def get_window(idx, total, window_size=5):\n",
    "    R = np.random.randint(1, window_size+1)\n",
    "    start = idx - R if (idx - R) > 0 else 0\n",
    "    stop = idx + R if (idx + R) < total else total\n",
    "\n",
    "    stock_int = get_ticker_int(idx)\n",
    "    stock_date = dates[idx]\n",
    "    \n",
    "    window = []\n",
    "    \n",
    "    for i in range(start, stop):\n",
    "        nearby_stock_int = get_ticker_int(i)\n",
    "        nearby_stock_date = dates[i]\n",
    "        if nearby_stock_int != stock_int and nearby_stock_date == stock_date:\n",
    "            window.append(nearby_stock_int)\n",
    "    \n",
    "    return window\n",
    "\n",
    "for idx in range(0, 20, 9):\n",
    "    print('window for', idx, tickers[idx], get_ticker_int(idx))\n",
    "    for nearby_int in get_window(idx, len(tickers), 5):\n",
    "        print(nearby_int, int_to_ticker[nearby_int])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "batch_size = 10000\n",
    "window_size = 10\n",
    "\n",
    "total_prices = len(prices)\n",
    "\n",
    "pbar = tqdm(total=int(total_prices / batch_size))\n",
    "\n",
    "def get_batch(start):\n",
    "    x, y = [], []\n",
    "\n",
    "    stop = start + batch_size if (start + batch_size) < total_prices else total_prices\n",
    "\n",
    "    for i in range(start, stop):\n",
    "        batch_x = get_ticker_int(i)\n",
    "        batch_y = get_window(i, total_prices, window_size)\n",
    "        y.extend(batch_y)\n",
    "        x.extend([batch_x]*len(batch_y))\n",
    "\n",
    "    pbar.update();\n",
    "\n",
    "    return [x, y]\n",
    "\n",
    "def get_batches():\n",
    "    batches = []\n",
    "    \n",
    "    for start in range(0, total_prices, batch_size):\n",
    "        batches.append(get_batch(start))\n",
    "   \n",
    "    return batches\n",
    "\n",
    "batches = get_batches()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save embedding metadata\n",
    "with open(STOCK_PATH, 'w') as out:\n",
    "  out.write('\\n'.join(int_to_ticker.values()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_stocks = len(df_prices['ticker'].unique())\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    inputs = tf.placeholder(tf.int32, [None], name='inputs')\n",
    "    labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "\n",
    "n_embedding = 400 # Number of embedding features \n",
    "\n",
    "with train_graph.as_default():\n",
    "    embedding = tf.Variable(tf.random_uniform((n_stocks, n_embedding), -1, 1), name='stock_embedding')\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of negative labels to sample\n",
    "n_sampled = 100\n",
    "\n",
    "with train_graph.as_default():\n",
    "    softmax_w = tf.Variable(tf.truncated_normal((n_stocks, n_embedding), stddev=0.1))\n",
    "    softmax_b = tf.Variable(tf.zeros(n_stocks))\n",
    "    \n",
    "    # Calculate the loss using negative sampling\n",
    "    loss = tf.nn.sampled_softmax_loss(softmax_w, softmax_b, \n",
    "                                      labels, embed,\n",
    "                                      n_sampled, n_stocks)\n",
    "    \n",
    "    cost = tf.reduce_mean(loss)\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with train_graph.as_default():\n",
    "    ## From Thushan Ganegedara's implementation\n",
    "    valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "    valid_window = 100\n",
    "    # pick 8 samples from (0,100) and (1000,1100) each ranges. lower id implies more frequent \n",
    "    valid_examples = np.array(random.sample(range(valid_window), valid_size//2))\n",
    "    valid_examples = np.append(valid_examples, \n",
    "                               random.sample(range(1000,1000+valid_window), valid_size//2))\n",
    "\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    \n",
    "    # We use the cosine distance:\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keep_dims=True))\n",
    "    normalized_embedding = embedding / norm\n",
    "    valid_embedding = tf.nn.embedding_lookup(normalized_embedding, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embedding, tf.transpose(normalized_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "epochs = 10\n",
    "\n",
    "with train_graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    iteration = 1\n",
    "    loss = 0\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for e in range(1, epochs+1):\n",
    "        start = time.time()\n",
    "        for batch in batches:\n",
    "            x = batch[0]\n",
    "            y = batch[1]\n",
    "            \n",
    "            feed = {inputs: x,\n",
    "                    labels: np.array(y)[:, None]}\n",
    "            train_loss, _ = sess.run([cost, optimizer], feed_dict=feed)\n",
    "            \n",
    "            loss += train_loss\n",
    "            \n",
    "            if iteration % 100 == 0: \n",
    "                end = time.time()\n",
    "                print(\"Epoch {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Avg. Training loss: {:.4f}\".format(loss/100),\n",
    "                      \"{:.4f} sec/batch\".format((end-start)/100))\n",
    "                loss = 0\n",
    "                start = time.time()\n",
    "            \n",
    "            if iteration % 10000 == 0:\n",
    "                # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "                sim = similarity.eval()\n",
    "                for i in range(valid_size):\n",
    "                    valid_stock = int_to_ticker[valid_examples[i]]\n",
    "                    top_k = 8 # number of nearest neighbors\n",
    "                    nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                    log = 'Nearest to %s:' % valid_stock\n",
    "                    for k in range(top_k):\n",
    "                        try:\n",
    "                            close_stock = int_to_ticker[nearest[k]]\n",
    "                            log = '%s %s,' % (log, close_stock)\n",
    "                        except Exception:\n",
    "                            print('nearest[k]', nearest[k])\n",
    "                    print(log)\n",
    "            \n",
    "            iteration += 1\n",
    "    save_path = saver.save(sess, MODEL_PATH)\n",
    "    embed_mat = sess.run(normalized_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save the embedding for tensorboard\n",
    "\n",
    "with train_graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    saver.restore(sess, MODEL_PATH)\n",
    "    \n",
    "    config = projector.ProjectorConfig()\n",
    "\n",
    "    viz_embedding = config.embeddings.add()\n",
    "    viz_embedding.tensor_name = embedding.name\n",
    "    viz_embedding.metadata_path = STOCK_PATH\n",
    "    summary_writer = tf.summary.FileWriter(LOG_DIR)\n",
    "    projector.visualize_embeddings(summary_writer, config)\n",
    "\n",
    "    saver.save(sess, MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "viz_stocks = 1000\n",
    "tsne = TSNE()\n",
    "embed_tsne = tsne.fit_transform(embed_mat[:viz_stocks, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20, 20))\n",
    "for idx in range(viz_stocks):\n",
    "    plt.scatter(*embed_tsne[idx, :], color='steelblue')\n",
    "    plt.annotate(int_to_ticker[idx], (embed_tsne[idx, 0], embed_tsne[idx, 1]), alpha=0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
