{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stock2vec\n",
    "\n",
    "Create vectors for stocks based on their relative volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1000/94011143 [00:00<00:06, 15141891.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      0           1     2\n",
      "229     AAAP_EPSDIL_MRT  2012-12-31 -0.76\n",
      "230     AAAP_EPSDIL_MRT  2013-12-31 -0.44\n",
      "231     AAAP_EPSDIL_MRT  2014-12-31 -0.30\n",
      "232     AAAP_EPSDIL_MRT  2015-12-31 -0.50\n",
      "1732    AABC_EPSDIL_MRT  2003-03-31  0.72\n",
      "1733    AABC_EPSDIL_MRT  2003-06-30  0.97\n",
      "1734    AABC_EPSDIL_MRT  2003-09-30  1.09\n",
      "1735    AABC_EPSDIL_MRT  2003-12-31  0.94\n",
      "1736    AABC_EPSDIL_MRT  2004-03-31  0.88\n",
      "1737    AABC_EPSDIL_MRT  2004-06-30  0.84\n",
      "1738    AABC_EPSDIL_MRT  2004-09-30  0.75\n",
      "1739    AABC_EPSDIL_MRT  2004-12-31  0.78\n",
      "1740    AABC_EPSDIL_MRT  2005-03-31  0.82\n",
      "1741    AABC_EPSDIL_MRT  2005-06-30  0.90\n",
      "1742    AABC_EPSDIL_MRT  2005-09-30  0.96\n",
      "7660    AACC_EPSDIL_MRT  2003-09-30  1.16\n",
      "7661    AACC_EPSDIL_MRT  2003-12-31  1.33\n",
      "7662    AACC_EPSDIL_MRT  2004-03-31 -0.03\n",
      "7663    AACC_EPSDIL_MRT  2004-06-30 -0.02\n",
      "7664    AACC_EPSDIL_MRT  2004-09-30 -0.03\n",
      "7665    AACC_EPSDIL_MRT  2004-12-31  0.02\n",
      "7666    AACC_EPSDIL_MRT  2005-03-31  1.41\n",
      "7667    AACC_EPSDIL_MRT  2005-06-30  1.54\n",
      "7668    AACC_EPSDIL_MRT  2005-09-30  1.57\n",
      "7669    AACC_EPSDIL_MRT  2005-12-31  1.38\n",
      "7670    AACC_EPSDIL_MRT  2006-03-31  1.32\n",
      "7671    AACC_EPSDIL_MRT  2006-06-30  1.21\n",
      "7672    AACC_EPSDIL_MRT  2006-09-30  1.13\n",
      "7673    AACC_EPSDIL_MRT  2006-12-31  1.24\n",
      "7674    AACC_EPSDIL_MRT  2007-03-31  1.18\n",
      "...                 ...         ...   ...\n",
      "292409  ABBV_EPSDIL_MRT  2013-09-30  2.84\n",
      "292410  ABBV_EPSDIL_MRT  2013-12-31  2.56\n",
      "292411  ABBV_EPSDIL_MRT  2014-03-31  2.57\n",
      "292412  ABBV_EPSDIL_MRT  2014-06-30  2.59\n",
      "292413  ABBV_EPSDIL_MRT  2014-09-30  2.30\n",
      "292414  ABBV_EPSDIL_MRT  2014-12-31  1.10\n",
      "292415  ABBV_EPSDIL_MRT  2015-03-31  1.12\n",
      "292416  ABBV_EPSDIL_MRT  2015-06-30  1.27\n",
      "292417  ABBV_EPSDIL_MRT  2015-09-30  1.70\n",
      "292418  ABBV_EPSDIL_MRT  2015-12-31  3.13\n",
      "292419  ABBV_EPSDIL_MRT  2016-03-31  3.32\n",
      "292420  ABBV_EPSDIL_MRT  2016-06-30  3.47\n",
      "292421  ABBV_EPSDIL_MRT  2016-09-30  3.70\n",
      "292422  ABBV_EPSDIL_MRT  2016-12-31  3.63\n",
      "299135   ABB_EPSDIL_MRT  2001-12-31 -0.66\n",
      "299136   ABB_EPSDIL_MRT  2002-12-31 -0.86\n",
      "299137   ABB_EPSDIL_MRT  2003-12-31 -0.64\n",
      "299138   ABB_EPSDIL_MRT  2004-12-31 -0.02\n",
      "299139   ABB_EPSDIL_MRT  2005-12-31  0.36\n",
      "299140   ABB_EPSDIL_MRT  2006-12-31  0.63\n",
      "299141   ABB_EPSDIL_MRT  2007-12-31  1.57\n",
      "299142   ABB_EPSDIL_MRT  2008-12-31  1.36\n",
      "299143   ABB_EPSDIL_MRT  2009-12-31  1.27\n",
      "299144   ABB_EPSDIL_MRT  2010-12-31  1.12\n",
      "299145   ABB_EPSDIL_MRT  2011-12-31  1.38\n",
      "299146   ABB_EPSDIL_MRT  2012-12-31  1.18\n",
      "299147   ABB_EPSDIL_MRT  2013-12-31  1.21\n",
      "299148   ABB_EPSDIL_MRT  2014-12-31  1.13\n",
      "299149   ABB_EPSDIL_MRT  2015-12-31  0.87\n",
      "299150   ABB_EPSDIL_MRT  2016-12-31  0.88\n",
      "\n",
      "[873 rows x 3 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "total_fundamentals_rows = 94011143\n",
    "\n",
    "df_eps = pd.read_csv('input/sharadar_fundamentals.csv', \n",
    "                     header=None,\n",
    "                     parse_dates=[0],\n",
    "                     chunksize=300000, iterator=True\n",
    "                    )\n",
    "\n",
    "df = df_eps.get_chunk()\n",
    "\n",
    "df = df[df[0].str.contains(\"EPSDIL_MRT\")]\n",
    "\n",
    "print(df)\n",
    "\n",
    "# TODO parse chunk and append to DF\n",
    "\n",
    "with tqdm(total=total_fundamentals_rows) as pbar:\n",
    "    pbar.update(1000)\n",
    "    \n",
    "# df.sort_values([1], inplace=True)\n",
    "\n",
    "# earliest_date = df.values[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ticker dimension    datekey  epsdil\n",
      "0   AAPL       ARQ 2001-08-13    0.01\n",
      "1   AAPL       ARQ 2001-12-21    0.01\n",
      "2   AAPL       ARQ 2002-02-11    0.01\n",
      "3   AAPL       ARQ 2002-05-14    0.01\n",
      "4   AAPL       ARQ 2002-08-09    0.01\n"
     ]
    }
   ],
   "source": [
    "# Import the earnings per share\n",
    "\n",
    "df_eps = pd.read_csv('input/sharadar_fundamentals.csv', \n",
    "                        usecols=[\"datekey\", \"dimension\", \"epsdil\", \"ticker\"],\n",
    "                        parse_dates=[\"datekey\"])\n",
    "\n",
    "df_eps.sort_values(['datekey', 'ticker', \"dimension\"], inplace=True)\n",
    "\n",
    "print(df_eps.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ticker       date  adj_close\n",
      "7561760     KO 1962-01-02   0.266600\n",
      "5620436     GE 1962-01-02   0.334451\n",
      "1005749   ARNC 1962-01-02   3.472967\n",
      "6739482    IBM 1962-01-02  15.561965\n",
      "1416507     BA 1962-01-02   0.480022\n"
     ]
    }
   ],
   "source": [
    "# Import the prices\n",
    "\n",
    "df_prices = pd.read_csv('input/wiki_prices.csv', \n",
    "                        usecols=[\"adj_close\", \"date\", \"ticker\"],\n",
    "                        parse_dates=['date'])\n",
    "\n",
    "df_prices.sort_values(['date', 'ticker'], inplace=True)\n",
    "\n",
    "df_prices = df_prices[pd.notnull(df_prices['adj_close'])]\n",
    "\n",
    "print(df_prices.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Filter ticks for the past ~15 years of stocks with volume > 10000 and volatility > 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_stocks 2866\n",
      "         ticker       date      volt\n",
      "1581020    BDBD 2016-01-04  0.000911\n",
      "1879551     BMR 2016-01-04  0.001688\n",
      "9523109    NTLS 2016-01-04  0.002186\n",
      "10150364    PCP 2016-01-04  0.002369\n",
      "14300884    WPP 2016-01-04  0.002446\n"
     ]
    }
   ],
   "source": [
    "df = df[df['date'] >= datetime.date(2016,1,1)]\n",
    "df = df[df['adj_volume'] > 10000]\n",
    "del df['adj_volume']\n",
    "\n",
    "df['volt'] = (df['adj_high'] - df['adj_low']) / df['adj_close']\n",
    "del df['adj_high']\n",
    "del df['adj_low']\n",
    "del df['adj_close']\n",
    "\n",
    "df = df[df['volt'] > 0]\n",
    "\n",
    "df.sort_values(['date', 'volt'], inplace=True)\n",
    "\n",
    "n_stocks = len(df['ticker'].unique())\n",
    "print('n_stocks', n_stocks)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get Nearby Stocks**\n",
    "\n",
    "For each stock, find C stocks that have the closest volatility to that ticker for that day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window for BDBD 0\n",
      "1 BMR\n",
      "2 NTLS\n",
      "3 PCP\n",
      "4 WPP\n",
      "window for CCG 5\n",
      "2 NTLS\n",
      "3 PCP\n",
      "4 WPP\n",
      "6 MDAS\n",
      "7 PBY\n"
     ]
    }
   ],
   "source": [
    "ticker_to_int = {}\n",
    "int_to_ticker = {}\n",
    "\n",
    "def get_ticker_int(ticker):\n",
    "    key = ticker_to_int.get(ticker, None)\n",
    "    if key is None:\n",
    "        key = ticker_to_int[ticker] = len(ticker_to_int)\n",
    "        int_to_ticker[key] = ticker\n",
    "    return key\n",
    "\n",
    "def get_stock_date(stocks, idx):\n",
    "    return stocks.iloc[[idx], 1].values[0]\n",
    "\n",
    "def get_stock_ticker(stocks, idx):\n",
    "    return stocks.iloc[[idx], 0].values[0]\n",
    "\n",
    "def get_stock_int(stocks, idx):\n",
    "    return get_ticker_int(get_stock_ticker(stocks, idx))\n",
    "\n",
    "def get_window(stocks, idx, window_size=5):\n",
    "    R = np.random.randint(1, window_size+1)\n",
    "    start = idx - R if (idx - R) > 0 else 0\n",
    "    stop = idx + R if (idx + R) < len(stocks) else len(stocks)\n",
    "\n",
    "    stock_int = get_stock_int(stocks, idx)\n",
    "    stock_date = get_stock_date(stocks, idx)\n",
    "    \n",
    "    window = []\n",
    "    \n",
    "    for i in range(start, stop):\n",
    "        nearby_stock_int = get_stock_int(stocks, i)\n",
    "        nearby_stock_date = get_stock_date(stocks, i)\n",
    "        if nearby_stock_int != stock_int and nearby_stock_date == stock_date:\n",
    "            window.append(nearby_stock_int)\n",
    "    \n",
    "    return window\n",
    "\n",
    "for idx in range(0, 10, 5):\n",
    "    print('window for', get_stock_ticker(df, idx), get_stock_int(df, idx))\n",
    "    for nearby_int in get_window(df, idx, 5):\n",
    "        print(nearby_int, int_to_ticker[nearby_int])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_batches(stocks, batch_size, window_size=5):    \n",
    "    for start in range(0, len(stocks), batch_size):\n",
    "        x, y = [], []\n",
    "        stop = start + batch_size if (start + batch_size) < len(stocks) else len(stocks)\n",
    "        \n",
    "        for i in range(start, stop):   \n",
    "            batch_x = get_stock_int(stocks, i)\n",
    "            batch_y = get_window(stocks, i, window_size)\n",
    "            y.extend(batch_y)\n",
    "            x.extend([batch_x]*len(batch_y))\n",
    "            \n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    inputs = tf.placeholder(tf.int32, [None], name='inputs')\n",
    "    labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "\n",
    "n_embedding = 400 # Number of embedding features \n",
    "\n",
    "with train_graph.as_default():\n",
    "    embedding = tf.Variable(tf.random_uniform((n_stocks, n_embedding), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of negative labels to sample\n",
    "n_sampled = 100\n",
    "\n",
    "with train_graph.as_default():\n",
    "    softmax_w = tf.Variable(tf.truncated_normal((n_stocks, n_embedding), stddev=0.1))\n",
    "    softmax_b = tf.Variable(tf.zeros(n_stocks))\n",
    "    \n",
    "    # Calculate the loss using negative sampling\n",
    "    loss = tf.nn.sampled_softmax_loss(softmax_w, softmax_b, \n",
    "                                      labels, embed,\n",
    "                                      n_sampled, n_stocks)\n",
    "    \n",
    "    cost = tf.reduce_mean(loss)\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with train_graph.as_default():\n",
    "    ## From Thushan Ganegedara's implementation\n",
    "    valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "    valid_window = 100\n",
    "    # pick 8 samples from (0,100) and (1000,1100) each ranges. lower id implies more frequent \n",
    "    valid_examples = np.array(random.sample(range(valid_window), valid_size//2))\n",
    "    valid_examples = np.append(valid_examples, \n",
    "                               random.sample(range(1000,1000+valid_window), valid_size//2))\n",
    "\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    \n",
    "    # We use the cosine distance:\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keep_dims=True))\n",
    "    normalized_embedding = embedding / norm\n",
    "    valid_embedding = tf.nn.embedding_lookup(normalized_embedding, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embedding, tf.transpose(normalized_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50 Iteration: 100 Avg. Training loss: 4.4161 3.3119 sec/batch\n",
      "Epoch 1/50 Iteration: 200 Avg. Training loss: 4.3350 3.1031 sec/batch\n",
      "Epoch 1/50 Iteration: 300 Avg. Training loss: 4.2741 3.1113 sec/batch\n",
      "Epoch 1/50 Iteration: 400 Avg. Training loss: 4.2321 3.1114 sec/batch\n",
      "Epoch 1/50 Iteration: 500 Avg. Training loss: 4.1339 3.0831 sec/batch\n",
      "Epoch 1/50 Iteration: 600 Avg. Training loss: 4.1440 3.0780 sec/batch\n",
      "Epoch 1/50 Iteration: 700 Avg. Training loss: 4.0824 3.0774 sec/batch\n",
      "Epoch 1/50 Iteration: 800 Avg. Training loss: 4.0500 3.0775 sec/batch\n",
      "Epoch 2/50 Iteration: 900 Avg. Training loss: 3.9751 3.0165 sec/batch\n",
      "Epoch 2/50 Iteration: 1000 Avg. Training loss: 3.8908 3.0740 sec/batch\n",
      "Nearest to PG: NLY, FGL, CKP, UMH, ETR, AROW, MDU, HTS,\n",
      "Nearest to CCG: BLT, CSGP, HW, FUR, PRE, NWBO, GRIF, UTX,\n",
      "Nearest to PMCS: GK, EAT, LKQ, COST, EXC, SGM, LVNTA, PVTB,\n",
      "Nearest to SJM: CVG, CNC, SLRC, COF, LMT, HOT, DOV, AVY,\n",
      "Nearest to POM: ASC, NILE, AOSL, UNT, PAYX, STRA, TFX, GPC,\n",
      "Nearest to PNY: PBY, TE, CBM, GAS, WTI, RSTI, CPGX, ACHC,\n",
      "Nearest to CVC: PNR, LADR, AAT, AME, HWKN, RCL, LTC, HAWK,\n",
      "Nearest to OVTI: WBMD, STWD, SYNT, AIG, PRA, DLTR, BNCN, JLL,\n",
      "Nearest to CDNS: SIR, MTD, DY, ABM, MIND, DHR, OFC, FPO,\n",
      "Nearest to PVH: DHR, IP, MKTX, RPTP, DUK, TRUE, YELP, PLPM,\n",
      "Nearest to MTN: CORT, PKD, ALX, UEC, SCM, MET, SNBC, ROP,\n",
      "Nearest to CNS: PLT, FIZZ, AEE, BRCD, WWAV, CSX, WWE, FE,\n",
      "Nearest to NEWM: VTNR, CTRN, UBNT, GSIG, LAD, SPLS, AGM, GRA,\n",
      "Nearest to KBR: AIRM, TFM, CNA, MEIP, MTOR, TUP, MDVN, ESRT,\n",
      "Nearest to CE: WGO, TGT, PCRX, MSM, SWX, ABCO, FIS, GLRI,\n",
      "Nearest to AMSG: NL, SCMP, WIX, LXRX, ALDR, SANM, BBT, CACC,\n",
      "Epoch 2/50 Iteration: 1100 Avg. Training loss: 3.8742 3.0666 sec/batch\n",
      "Epoch 2/50 Iteration: 1200 Avg. Training loss: 3.8376 3.0660 sec/batch\n",
      "Epoch 2/50 Iteration: 1300 Avg. Training loss: 3.8392 3.0585 sec/batch\n",
      "Epoch 2/50 Iteration: 1400 Avg. Training loss: 3.8456 3.0709 sec/batch\n",
      "Epoch 2/50 Iteration: 1500 Avg. Training loss: 3.8171 3.0912 sec/batch\n",
      "Epoch 2/50 Iteration: 1600 Avg. Training loss: 3.8134 3.0869 sec/batch\n",
      "Epoch 3/50 Iteration: 1700 Avg. Training loss: 3.8359 2.9442 sec/batch\n",
      "Epoch 3/50 Iteration: 1800 Avg. Training loss: 3.7384 3.0683 sec/batch\n",
      "Epoch 3/50 Iteration: 1900 Avg. Training loss: 3.7226 3.0840 sec/batch\n",
      "Epoch 3/50 Iteration: 2000 Avg. Training loss: 3.7040 3.0782 sec/batch\n",
      "Nearest to PG: NLY, FGL, ETR, HTS, CKP, MDU, UMH, EQY,\n",
      "Nearest to CCG: BLT, PRE, FUR, CSGP, UTX, GRIF, ADT, GMCR,\n",
      "Nearest to PMCS: GK, EAT, LKQ, COST, EXC, PVTB, MDT, LVNTA,\n",
      "Nearest to SJM: CVG, CNC, HOT, AVY, LMT, SLRC, EMC, BIO,\n",
      "Nearest to POM: ASC, NILE, UNT, AOSL, PAYX, BLL, STRA, TFX,\n",
      "Nearest to PNY: TE, GAS, RSTI, PBY, CPGX, DWA, MKTO, STR,\n",
      "Nearest to CVC: PNR, LADR, STR, AAT, AME, GAS, TUMI, SQI,\n",
      "Nearest to OVTI: WBMD, STWD, SYNT, PRA, AIG, BSX, JLL, HTA,\n",
      "Nearest to CDNS: SIR, MTD, ABM, DHR, OFC, APD, CAB, MIND,\n",
      "Nearest to PVH: DHR, IP, MKTX, DUK, RPTP, TRUE, BELFB, YELP,\n",
      "Nearest to MTN: CORT, ALX, PKD, GBDC, ROP, SCM, UEC, HASI,\n",
      "Nearest to CNS: PLT, FIZZ, AEE, CSX, BRCD, EQIX, FE, WWE,\n",
      "Nearest to NEWM: CTRN, UBNT, GSIG, SPLS, VTNR, LAD, AGM, GRA,\n",
      "Nearest to KBR: AIRM, MEIP, CNA, TFM, MTOR, MDVN, TNGO, MASI,\n",
      "Nearest to CE: WGO, TGT, MSM, SWX, FIS, LEA, PCRX, CGNX,\n",
      "Nearest to AMSG: NL, SCMP, WIX, BBT, CACC, CPLA, IDTI, CFR,\n",
      "Epoch 3/50 Iteration: 2100 Avg. Training loss: 3.7313 3.0600 sec/batch\n",
      "Epoch 3/50 Iteration: 2200 Avg. Training loss: 3.7897 3.0601 sec/batch\n",
      "Epoch 3/50 Iteration: 2300 Avg. Training loss: 3.7111 3.1097 sec/batch\n",
      "Epoch 3/50 Iteration: 2400 Avg. Training loss: 3.7299 3.0994 sec/batch\n",
      "Epoch 4/50 Iteration: 2500 Avg. Training loss: 3.7718 2.9134 sec/batch\n",
      "Epoch 4/50 Iteration: 2600 Avg. Training loss: 3.6931 3.1271 sec/batch\n",
      "Epoch 4/50 Iteration: 2700 Avg. Training loss: 3.7013 3.1176 sec/batch\n",
      "Epoch 4/50 Iteration: 2800 Avg. Training loss: 3.6896 3.2323 sec/batch\n",
      "Epoch 4/50 Iteration: 2900 Avg. Training loss: 3.6988 3.1732 sec/batch\n",
      "Epoch 4/50 Iteration: 3000 Avg. Training loss: 3.7216 3.1205 sec/batch\n",
      "Nearest to PG: FGL, NLY, ETR, CKP, HTS, MDU, UMH, CIR,\n",
      "Nearest to CCG: BLT, PRE, FUR, GMCR, RLD, PCP, IRC, ADT,\n",
      "Nearest to PMCS: GK, EAT, PVTB, COST, LKQ, EXC, MDT, CPA,\n",
      "Nearest to SJM: CVG, CNC, HOT, EMC, LMT, ATML, COF, AVY,\n",
      "Nearest to POM: ASC, NILE, UNT, AOSL, PAYX, BLL, STRA, SGNT,\n",
      "Nearest to PNY: RSTI, TE, PBY, CPGX, GAS, DWA, SGI, STR,\n",
      "Nearest to CVC: PNR, LADR, GAS, STR, AME, AAT, TUMI, SQI,\n",
      "Nearest to OVTI: WBMD, STWD, AIG, PRA, SYNT, RLD, MDAS, JLL,\n",
      "Nearest to CDNS: SIR, ABM, MTD, MIND, CAB, DHR, APD, OFC,\n",
      "Nearest to PVH: DHR, IP, DUK, MKTX, RPTP, TRUE, BELFB, HBHC,\n",
      "Nearest to MTN: CORT, PKD, UEC, SCM, ALX, GBDC, ROP, DXCM,\n",
      "Nearest to CNS: PLT, FIZZ, AEE, CSX, WWE, FE, RVLT, EQIX,\n",
      "Nearest to NEWM: CTRN, VTNR, UBNT, GSIG, SPLS, LAD, GRA, AGM,\n",
      "Nearest to KBR: AIRM, MEIP, CNA, MTOR, MASI, TFM, TNGO, TUP,\n",
      "Nearest to CE: WGO, TGT, PCRX, SWX, MSM, ABCO, LEA, FIS,\n",
      "Nearest to AMSG: NL, SCMP, WIX, CACC, CPLA, BBT, QTWO, IDTI,\n",
      "Epoch 4/50 Iteration: 3100 Avg. Training loss: 3.6647 3.0656 sec/batch\n",
      "Epoch 4/50 Iteration: 3200 Avg. Training loss: 3.6764 3.0787 sec/batch\n",
      "Epoch 5/50 Iteration: 3300 Avg. Training loss: 3.7427 2.8221 sec/batch\n",
      "Epoch 5/50 Iteration: 3400 Avg. Training loss: 3.6549 3.0761 sec/batch\n",
      "Epoch 5/50 Iteration: 3500 Avg. Training loss: 3.6414 3.0685 sec/batch\n",
      "Epoch 5/50 Iteration: 3600 Avg. Training loss: 3.6292 3.0742 sec/batch\n",
      "Epoch 5/50 Iteration: 3700 Avg. Training loss: 3.6443 3.0673 sec/batch\n",
      "Epoch 5/50 Iteration: 3800 Avg. Training loss: 3.6831 3.1279 sec/batch\n",
      "Epoch 5/50 Iteration: 3900 Avg. Training loss: 3.6515 3.1858 sec/batch\n",
      "Epoch 5/50 Iteration: 4000 Avg. Training loss: 3.6682 3.0904 sec/batch\n",
      "Nearest to PG: NLY, FGL, ETR, CKP, UMH, MDU, HTS, CIR,\n",
      "Nearest to CCG: BLT, PRE, FUR, PCP, RLD, CSGP, IRC, ADT,\n",
      "Nearest to PMCS: GK, EAT, PVTB, COST, LKQ, EXC, CPA, MDT,\n",
      "Nearest to SJM: CNC, CVG, HOT, COF, DOV, LMT, EMC, ATML,\n",
      "Nearest to POM: NILE, ASC, AOSL, UNT, PAYX, STRA, SGNT, BLL,\n",
      "Nearest to PNY: RSTI, TE, PBY, DWA, GAS, CPGX, STR, SGI,\n",
      "Nearest to CVC: PNR, LADR, GAS, STR, TUMI, SQI, AAT, AME,\n",
      "Nearest to OVTI: WBMD, STWD, PRA, AIG, RLD, SYNT, MDAS, JLL,\n",
      "Nearest to CDNS: SIR, ABM, MTD, CAB, DHR, APD, MIND, FPO,\n",
      "Nearest to PVH: DHR, IP, RPTP, TRUE, MKTX, BELFB, YELP, DUK,\n",
      "Nearest to MTN: CORT, PKD, ALX, UEC, SCM, GBDC, ROP, HASI,\n",
      "Nearest to CNS: PLT, FIZZ, AEE, CSX, WWE, FE, EQIX, ACRE,\n",
      "Nearest to NEWM: CTRN, VTNR, GSIG, UBNT, GRA, AEE, RECN, FIS,\n",
      "Nearest to KBR: AIRM, MEIP, CNA, MTOR, MASI, MDVN, TFM, TUP,\n",
      "Nearest to CE: WGO, TGT, LEA, MSM, SWX, EMN, EW, FIS,\n",
      "Nearest to AMSG: NL, SCMP, WIX, QTWO, CACC, BBT, CPLA, ALDR,\n",
      "Epoch 6/50 Iteration: 4100 Avg. Training loss: 3.7251 2.8030 sec/batch\n",
      "Epoch 6/50 Iteration: 4200 Avg. Training loss: 3.6322 3.0758 sec/batch\n",
      "Epoch 6/50 Iteration: 4300 Avg. Training loss: 3.6314 3.6040 sec/batch\n",
      "Epoch 6/50 Iteration: 4400 Avg. Training loss: 3.6328 3.4781 sec/batch\n",
      "Epoch 6/50 Iteration: 4500 Avg. Training loss: 3.6394 3.0529 sec/batch\n",
      "Epoch 6/50 Iteration: 4600 Avg. Training loss: 3.6778 3.0433 sec/batch\n",
      "Epoch 6/50 Iteration: 4700 Avg. Training loss: 3.6459 3.0562 sec/batch\n",
      "Epoch 6/50 Iteration: 4800 Avg. Training loss: 3.6600 3.0468 sec/batch\n",
      "Epoch 7/50 Iteration: 4900 Avg. Training loss: 3.7180 2.6895 sec/batch\n",
      "Epoch 7/50 Iteration: 5000 Avg. Training loss: 3.6465 3.0501 sec/batch\n",
      "Nearest to PG: FGL, NLY, ETR, UMH, CKP, CIR, MDU, DUK,\n",
      "Nearest to CCG: BLT, PRE, FUR, GMCR, PCP, UTX, IRC, RLD,\n",
      "Nearest to PMCS: GK, EAT, COST, PVTB, EXC, TJX, MDT, IRC,\n",
      "Nearest to SJM: CVG, CNC, HOT, COF, DOV, ATML, LMT, CBM,\n",
      "Nearest to POM: NILE, ASC, AOSL, UNT, STRA, PAYX, TFX, BLL,\n",
      "Nearest to PNY: PBY, TE, GAS, RSTI, CBM, STR, CPGX, TSN,\n",
      "Nearest to CVC: PNR, LADR, AAT, GAS, AME, STR, ARG, HWKN,\n",
      "Nearest to OVTI: WBMD, PRA, AIG, RLD, SYNT, STWD, UBA, SLH,\n",
      "Nearest to CDNS: ABM, SIR, MTD, CAB, APD, DHR, FPO, INO,\n",
      "Nearest to PVH: DHR, IP, RPTP, DUK, MKTX, TRUE, BELFB, YELP,\n",
      "Nearest to MTN: CORT, UEC, PKD, ALX, SCM, GBDC, HASI, DXCM,\n",
      "Nearest to CNS: PLT, AEE, FIZZ, FE, CSX, WWE, ACRE, EQIX,\n",
      "Nearest to NEWM: CTRN, VTNR, UBNT, GSIG, GRA, AEE, FIS, LAD,\n",
      "Nearest to KBR: AIRM, MEIP, CNA, MASI, TFM, MTOR, MDVN, MYGN,\n",
      "Nearest to CE: WGO, TGT, LEA, MSM, SWX, EW, EMN, ABCO,\n",
      "Nearest to AMSG: SCMP, NL, WIX, QTWO, BBT, CPLA, CACC, ADM,\n",
      "Epoch 7/50 Iteration: 5100 Avg. Training loss: 3.6436 3.1480 sec/batch\n",
      "Epoch 7/50 Iteration: 5200 Avg. Training loss: 3.6117 3.0574 sec/batch\n",
      "Epoch 7/50 Iteration: 5300 Avg. Training loss: 3.6398 3.0573 sec/batch\n",
      "Epoch 7/50 Iteration: 5400 Avg. Training loss: 3.6758 3.0503 sec/batch\n",
      "Epoch 7/50 Iteration: 5500 Avg. Training loss: 3.6305 3.0547 sec/batch\n",
      "Epoch 7/50 Iteration: 5600 Avg. Training loss: 3.6473 3.0486 sec/batch\n",
      "Epoch 8/50 Iteration: 5700 Avg. Training loss: 3.7210 2.6274 sec/batch\n",
      "Epoch 8/50 Iteration: 5800 Avg. Training loss: 3.6138 3.0493 sec/batch\n",
      "Epoch 8/50 Iteration: 5900 Avg. Training loss: 3.6203 3.0520 sec/batch\n",
      "Epoch 8/50 Iteration: 6000 Avg. Training loss: 3.6190 3.0468 sec/batch\n",
      "Nearest to PG: FGL, NLY, ETR, UMH, CKP, CIR, MDU, HTS,\n",
      "Nearest to CCG: BLT, PRE, FUR, RLD, IRC, PCP, UTX, GMCR,\n",
      "Nearest to PMCS: EAT, GK, COST, PVTB, EXC, TJX, MDT, ELRC,\n",
      "Nearest to SJM: CVG, CNC, DOV, HOT, COF, DXLG, ATML, LMT,\n",
      "Nearest to POM: ASC, NILE, AOSL, UNT, STRA, PAYX, CSGP, DVA,\n",
      "Nearest to PNY: RSTI, TE, PBY, CBM, GAS, STR, CPGX, DWA,\n",
      "Nearest to CVC: PNR, LADR, GAS, TUMI, AAT, STR, AME, ARG,\n",
      "Nearest to OVTI: WBMD, PRA, RLD, AIG, SYNT, STWD, UBA, SLH,\n",
      "Nearest to CDNS: ABM, SIR, APD, MTD, FPO, DHR, CAB, INO,\n",
      "Nearest to PVH: DHR, IP, RPTP, DUK, YELP, MKTX, TRUE, BELFB,\n",
      "Nearest to MTN: CORT, UEC, ALX, PKD, SCM, GBDC, DXCM, ECOL,\n",
      "Nearest to CNS: PLT, AEE, FE, FIZZ, CSX, WWE, EQIX, ACRE,\n",
      "Nearest to NEWM: CTRN, VTNR, GRA, AEE, UBNT, GSIG, FIS, CFR,\n",
      "Nearest to KBR: AIRM, MEIP, MDVN, MASI, CNA, MYGN, MTOR, TFM,\n",
      "Nearest to CE: WGO, TGT, MSM, LEA, SWX, EW, EMN, ABCO,\n",
      "Nearest to AMSG: SCMP, NL, WIX, BBT, QTWO, CACC, CPLA, ADM,\n",
      "Epoch 8/50 Iteration: 6100 Avg. Training loss: 3.6094 3.0601 sec/batch\n",
      "Epoch 8/50 Iteration: 6200 Avg. Training loss: 3.6468 3.0463 sec/batch\n",
      "Epoch 8/50 Iteration: 6300 Avg. Training loss: 3.6145 3.0442 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "batch_size = 1000\n",
    "window_size = 5\n",
    "\n",
    "with train_graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    iteration = 1\n",
    "    loss = 0\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for e in range(1, epochs+1):\n",
    "        batches = get_batches(df, batch_size, window_size)\n",
    "        start = time.time()\n",
    "        for x, y in batches:           \n",
    "            feed = {inputs: x,\n",
    "                    labels: np.array(y)[:, None]}\n",
    "            train_loss, _ = sess.run([cost, optimizer], feed_dict=feed)\n",
    "            \n",
    "            loss += train_loss\n",
    "            \n",
    "            if iteration % 100 == 0: \n",
    "                end = time.time()\n",
    "                print(\"Epoch {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Avg. Training loss: {:.4f}\".format(loss/100),\n",
    "                      \"{:.4f} sec/batch\".format((end-start)/100))\n",
    "                loss = 0\n",
    "                start = time.time()\n",
    "            \n",
    "            if iteration % 1000 == 0:\n",
    "                # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "                sim = similarity.eval()\n",
    "                for i in range(valid_size):\n",
    "                    valid_stock = int_to_ticker[valid_examples[i]]\n",
    "                    top_k = 8 # number of nearest neighbors\n",
    "                    nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                    log = 'Nearest to %s:' % valid_stock\n",
    "                    for k in range(top_k):\n",
    "                        try:\n",
    "                            close_stock = int_to_ticker[nearest[k]]\n",
    "                            log = '%s %s,' % (log, close_stock)\n",
    "                        except Exception:\n",
    "                            print('nearest[k]', nearest[k])\n",
    "                    print(log)\n",
    "            \n",
    "            iteration += 1\n",
    "    save_path = saver.save(sess, \"checkpoints/text8.ckpt\")\n",
    "    embed_mat = sess.run(normalized_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
