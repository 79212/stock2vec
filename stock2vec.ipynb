{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stock2vec\n",
    "\n",
    "Create vectors for stocks based on their relative volatility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        ticker       date   adj_high    adj_low  adj_close  adj_volume\n",
      "7561760     KO 1962-01-02   0.273859   0.266600   0.266600   1612800.0\n",
      "5620436     GE 1962-01-02   0.341163   0.332214   0.334451   2073600.0\n",
      "1005749   ARNC 1962-01-02   3.492621   3.472967   3.472967     44800.0\n",
      "6739482    IBM 1962-01-02  15.738806  15.561965  15.561965    387200.0\n",
      "1416507     BA 1962-01-02   0.488470   0.480022   0.480022    352200.0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('data/wiki_prices.csv', \n",
    "                        usecols=[\"ticker\", \"date\", \"adj_close\", \"adj_high\", \"adj_low\", \"adj_volume\"],\n",
    "                        parse_dates=['date'])\n",
    "\n",
    "df.sort_values('date', inplace=True)\n",
    "\n",
    "df = df[pd.notnull(df['adj_high']) & pd.notnull(df['adj_low']) & pd.notnull(df['adj_close'])]\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "\n",
    "Filter ticks for the past ~15 years of stocks with volume > 10000 and volatility > 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_stocks 3186\n",
      "         ticker       date      volt\n",
      "3017202   CNBKA 2000-01-03  0.003679\n",
      "1872359     BMI 2000-01-03  0.004000\n",
      "5209067    FLIC 2000-01-03  0.004068\n",
      "5368021    FRED 2000-01-03  0.004391\n",
      "12380997    STL 2000-01-03  0.004499\n"
     ]
    }
   ],
   "source": [
    "df = df[df['date'] >= datetime.date(2000,1,1)]\n",
    "df = df[df['adj_volume'] > 10000]\n",
    "del df['adj_volume']\n",
    "\n",
    "df['volt'] = (df['adj_high'] - df['adj_low']) / df['adj_close']\n",
    "del df['adj_high']\n",
    "del df['adj_low']\n",
    "del df['adj_close']\n",
    "\n",
    "df = df[df['volt'] > 0]\n",
    "\n",
    "df.sort_values(['date', 'volt'], inplace=True)\n",
    "\n",
    "n_stocks = len(df['ticker'].unique())\n",
    "print('n_stocks', n_stocks)\n",
    "\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get Nearby Stocks**\n",
    "\n",
    "For each stock, find C stocks that have the closest volatility to that ticker for that day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window for CNBKA 0\n",
      "1 BMI\n",
      "2 FLIC\n",
      "3 FRED\n",
      "4 STL\n",
      "5 WEYS\n",
      "window for WEYS 5\n",
      "0 CNBKA\n",
      "1 BMI\n",
      "2 FLIC\n",
      "3 FRED\n",
      "4 STL\n",
      "6 UMBF\n",
      "7 CRRC\n",
      "8 THO\n",
      "9 MAC\n",
      "10 MAS\n"
     ]
    }
   ],
   "source": [
    "ticker_to_int = {}\n",
    "int_to_ticker = {}\n",
    "\n",
    "def get_ticker_int(ticker):\n",
    "    key = ticker_to_int.get(ticker, None)\n",
    "    if key is None:\n",
    "        key = ticker_to_int[ticker] = len(ticker_to_int)\n",
    "        int_to_ticker[key] = ticker\n",
    "    return key\n",
    "\n",
    "def get_stock_date(stocks, idx):\n",
    "    return stocks.iloc[[idx], 1].values[0]\n",
    "\n",
    "def get_stock_ticker(stocks, idx):\n",
    "    return stocks.iloc[[idx], 0].values[0]\n",
    "\n",
    "def get_stock_int(stocks, idx):\n",
    "    return get_ticker_int(get_stock_ticker(stocks, idx))\n",
    "\n",
    "def get_window(stocks, idx, window_size=5):\n",
    "    R = np.random.randint(1, window_size+1)\n",
    "    start = idx - R if (idx - R) > 0 else 0\n",
    "    stop = idx + R\n",
    "    \n",
    "    stock_int = get_stock_int(stocks, idx)\n",
    "    stock_date = get_stock_date(stocks, idx)\n",
    "    \n",
    "    window = []\n",
    "    \n",
    "    for i in range(start, stop+1):\n",
    "        nearby_stock_int = get_stock_int(stocks, i)\n",
    "        nearby_stock_date = get_stock_date(stocks, i)\n",
    "        if nearby_stock_int != stock_int and nearby_stock_date == stock_date:\n",
    "            window.append(nearby_stock_int)\n",
    "    \n",
    "    return window\n",
    "\n",
    "for idx in range(0, 10, 5):\n",
    "    print('window for', get_stock_ticker(df, idx), get_stock_int(df, idx))\n",
    "    for nearby_int in get_window(df, idx, 5):\n",
    "        print(nearby_int, int_to_ticker[nearby_int])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_batches(stocks, batch_size, window_size=5):    \n",
    "    for start in range(0, len(stocks), batch_size):\n",
    "        x, y = [], []\n",
    "        stop = start + batch_size if (start + batch_size) < len(stocks) else len(stocks)\n",
    "        \n",
    "        for i in range(start, stop):   \n",
    "            batch_x = get_stock_int(stocks, i)\n",
    "            batch_y = get_window(stocks, i, window_size)\n",
    "            y.extend(batch_y)\n",
    "            x.extend([batch_x]*len(batch_y))\n",
    "            \n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    inputs = tf.placeholder(tf.int32, [None], name='inputs')\n",
    "    labels = tf.placeholder(tf.int32, [None, None], name='labels')\n",
    "\n",
    "n_embedding = 300 # Number of embedding features \n",
    "\n",
    "with train_graph.as_default():\n",
    "    embedding = tf.Variable(tf.random_uniform((n_stocks, n_embedding), -1, 1))\n",
    "    embed = tf.nn.embedding_lookup(embedding, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Negative sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Number of negative labels to sample\n",
    "n_sampled = 100\n",
    "\n",
    "with train_graph.as_default():\n",
    "    softmax_w = tf.Variable(tf.truncated_normal((n_stocks, n_embedding), stddev=0.1))\n",
    "    softmax_b = tf.Variable(tf.zeros(n_stocks))\n",
    "    \n",
    "    # Calculate the loss using negative sampling\n",
    "    loss = tf.nn.sampled_softmax_loss(softmax_w, softmax_b, \n",
    "                                      labels, embed,\n",
    "                                      n_sampled, n_stocks)\n",
    "    \n",
    "    cost = tf.reduce_mean(loss)\n",
    "    optimizer = tf.train.AdamOptimizer().minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with train_graph.as_default():\n",
    "    ## From Thushan Ganegedara's implementation\n",
    "    valid_size = 16 # Random set of words to evaluate similarity on.\n",
    "    valid_window = 100\n",
    "    # pick 8 samples from (0,100) and (1000,1100) each ranges. lower id implies more frequent \n",
    "    valid_examples = np.array(random.sample(range(valid_window), valid_size//2))\n",
    "    valid_examples = np.append(valid_examples, \n",
    "                               random.sample(range(1000,1000+valid_window), valid_size//2))\n",
    "\n",
    "    valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "    \n",
    "    # We use the cosine distance:\n",
    "    norm = tf.sqrt(tf.reduce_sum(tf.square(embedding), 1, keep_dims=True))\n",
    "    normalized_embedding = embedding / norm\n",
    "    valid_embedding = tf.nn.embedding_lookup(normalized_embedding, valid_dataset)\n",
    "    similarity = tf.matmul(valid_embedding, tf.transpose(normalized_embedding))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 Iteration: 1 Avg. Training loss: 0.0517 0.0408 sec/batch\n",
      "Epoch 1/10 Iteration: 2 Avg. Training loss: 0.0482 0.0391 sec/batch\n",
      "Epoch 1/10 Iteration: 3 Avg. Training loss: 0.0450 0.0376 sec/batch\n",
      "Epoch 1/10 Iteration: 4 Avg. Training loss: 0.0515 0.0370 sec/batch\n",
      "Epoch 1/10 Iteration: 5 Avg. Training loss: 0.0503 0.0378 sec/batch\n",
      "Epoch 1/10 Iteration: 6 Avg. Training loss: 0.0503 0.0377 sec/batch\n",
      "Epoch 1/10 Iteration: 7 Avg. Training loss: 0.0512 0.0374 sec/batch\n",
      "Epoch 1/10 Iteration: 8 Avg. Training loss: 0.0506 0.0377 sec/batch\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "batch_size = 1000\n",
    "window_size = 5\n",
    "\n",
    "with train_graph.as_default():\n",
    "    saver = tf.train.Saver()\n",
    "\n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    iteration = 1\n",
    "    loss = 0\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    for e in range(1, epochs+1):\n",
    "        batches = get_batches(df, batch_size, window_size)\n",
    "        start = time.time()\n",
    "        for x, y in batches:           \n",
    "            feed = {inputs: x,\n",
    "                    labels: np.array(y)[:, None]}\n",
    "            train_loss, _ = sess.run([cost, optimizer], feed_dict=feed)\n",
    "            \n",
    "            loss += train_loss\n",
    "            \n",
    "            if iteration % 100 == 0: \n",
    "                end = time.time()\n",
    "                print(\"Epoch {}/{}\".format(e, epochs),\n",
    "                      \"Iteration: {}\".format(iteration),\n",
    "                      \"Avg. Training loss: {:.4f}\".format(loss/100),\n",
    "                      \"{:.4f} sec/batch\".format((end-start)/100))\n",
    "                loss = 0\n",
    "                start = time.time()\n",
    "            \n",
    "            if iteration % 1000 == 0:\n",
    "                # note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "                sim = similarity.eval()\n",
    "                for i in range(valid_size):\n",
    "                    valid_word = int_to_stock[valid_examples[i]]\n",
    "                    top_k = 8 # number of nearest neighbors\n",
    "                    nearest = (-sim[i, :]).argsort()[1:top_k+1]\n",
    "                    log = 'Nearest to %s:' % valid_word\n",
    "                    for k in range(top_k):\n",
    "                        close_word = int_to_vocab[nearest[k]]\n",
    "                        log = '%s %s,' % (log, close_word)\n",
    "                    print(log)\n",
    "            \n",
    "            iteration += 1\n",
    "    save_path = saver.save(sess, \"checkpoints/text8.ckpt\")\n",
    "    embed_mat = sess.run(normalized_embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
